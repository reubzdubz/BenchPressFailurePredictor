# -*- coding: utf-8 -*-
"""LSTM on GYM data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tg6N1SquSb0sS8bZdgZLPLzXM2Y75LyI
"""


import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import joblib

# Set random seeds for reproducibility
torch.manual_seed(42)
np.random.seed(42)

# Load the Excel file
df = pd.read_excel('signalSplit.xlsx')
print(f"Data shape: {df.shape}")

def normalize_signals(df):
    """
    Normalize each signal column individually to [0, 1] range.

    Args:
        df (pd.DataFrame): DataFrame containing signal columns (e.g., 'signal1_1', 'signal1_2', ...)

    Returns:
        pd.DataFrame: DataFrame with normalized signal values.
    """
    normalized_df = df.copy()
    for col in df.columns:
        col_min = df[col].min()
        col_max = df[col].max()
        # Avoid division by zero if constant values
        if col_max > col_min:
            normalized_df[col] = (df[col] - col_min) / (col_max - col_min)
        else:
            normalized_df[col] = df[col]  # leave unchanged if all values are the same
    return normalized_df


# Step 1: Data preprocessing and preparation
def prepare_sequences(df):
    """
    Prepare sequences from the dataframe for LSTM training
    Each signal represents a complete rep sequence
    """
    signal_data = []
    failure_signals = [5, 6, 11, 12, 19, 20, 25, 26, 30, 31, 37, 38, 43, 44, 52, 53, 61, 62, 66, 67, 74, 75, 80, 81, 87, 88, 92, 93, 100, 101, 103, 104, 112, 113, 114, 119, 120, 127, 128, 131, 132, 138, 139, 141, 142, 143, 144]  # Signals that indicate failure

    # We have signals from signal1 to signal31 (based on your data)
    for i in range(1, 145):  # 1 to 31
        col1 = f'signal{i}_1'  # arm angle
        col2 = f'signal{i}_2'  # wrist y position

        if col1 in df.columns and col2 in df.columns:
            # Get non-null values for this signal pair
            signal_pair = df[[col1, col2]].dropna()

            if len(signal_pair) > 0:
                # Label: 1 if failure, 0 if non-failure
                label = 1 if i in failure_signals else 0

                signal_data.append({
                    'signal_id': i,
                    'data': signal_pair.values,
                    'length': len(signal_pair),
                    'label': label
                })

    return signal_data



# Step 2: Left padding and sequence creation
def create_padded_sequences(signal_data):
    """
    Left-pad sequences with zeros and create training data
    """
    # Find maximum length for padding
    max_length = max([s['length'] for s in signal_data])
    print(f"Maximum sequence length: {max_length}")

    padded_sequences = []
    labels = []

    for signal in signal_data:
        data = signal['data']
        length = signal['length']

        # Left pad with zeros
        padding_length = max_length - length
        if padding_length > 0:
            padding = np.zeros((padding_length, 2))
            padded_data = np.vstack([padding, data])
        else:
            padded_data = data

        padded_sequences.append(padded_data)
        labels.append(signal['label'])

    return np.array(padded_sequences), np.array(labels), max_length

# Step 3: Custom Dataset class for PyTorch
class BenchPressDataset(Dataset):
    def __init__(self, X, y):
        self.X = torch.FloatTensor(X)
        self.y = torch.LongTensor(y)  # Changed to LongTensor for classification

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

# Step 4: LSTM Model Definition for Classification
class BenchPressLSTM(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, num_classes=2, dropout=0.2):
        super(BenchPressLSTM, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers

        # LSTM encoder
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers,
                           batch_first=True, dropout=dropout if num_layers > 1 else 0)

        # Dropout layer
        self.dropout = nn.Dropout(dropout)

        # Dense layers to map to class logits
        self.fc1 = nn.Linear(hidden_size, hidden_size // 2)
        self.fc2 = nn.Linear(hidden_size // 2, num_classes)
        self.relu = nn.ReLU()

    def forward(self, x):
        # LSTM forward pass
        lstm_out, (h_n, c_n) = self.lstm(x)

        # Use the last hidden state
        last_hidden = h_n[-1]  # Take the last layer's hidden state

        # Apply dropout
        out = self.dropout(last_hidden)

        # Dense layers
        out = self.relu(self.fc1(out))
        out = self.fc2(out)

        return out

# Execute data preparation
signal_data = prepare_sequences(df)
print(f"Found {len(signal_data)} valid signals")

padded_sequences, labels, max_length = create_padded_sequences(signal_data)
print(f"Padded sequences shape: {padded_sequences.shape}")
print(f"Labels shape: {labels.shape}")
print(f"Class distribution - Non-failure: {np.sum(labels == 0)}, Failure: {np.sum(labels == 1)}")

# Normalize the data
scaler_X = StandardScaler()

# Reshape for scaling
n_samples, seq_len, n_features = padded_sequences.shape
X_reshaped = padded_sequences.reshape(-1, n_features)
X_scaled = scaler_X.fit_transform(X_reshaped).reshape(n_samples, seq_len, n_features)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, labels, test_size=0.5, random_state=42, stratify=labels
)

print(f"Training data shape: X={X_train.shape}, y={y_train.shape}")
print(f"Test data shape: X={X_test.shape}, y={y_test.shape}")
print(f"Training class distribution - Non-failure: {np.sum(y_train == 0)}, Failure: {np.sum(y_train == 1)}")
print(f"Test class distribution - Non-failure: {np.sum(y_test == 0)}, Failure: {np.sum(y_test == 1)}")

# Create datasets and dataloaders
train_dataset = BenchPressDataset(X_train, y_train)
test_dataset = BenchPressDataset(X_test, y_test)

train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)

# Model parameters
input_size = n_features  # 2 features (arm angle, wrist y position)
hidden_size = 128
num_layers = 4
num_classes = 2  # Binary classification: 0=non-failure, 1=failure
dropout = 0.2

# Initialize model
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

model = BenchPressLSTM(input_size, hidden_size, num_layers, num_classes, dropout)
model.to(device)

# Loss function and optimizer
# Use class weights to handle imbalanced data
class_counts = np.bincount(y_train)
class_weights = torch.FloatTensor([1.0 / count for count in class_counts]).to(device)
criterion = nn.CrossEntropyLoss(weight=class_weights)

optimizer = optim.Adam(model.parameters(), lr=0.001)
scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.5)

# Training function
def train_model(model, train_loader, test_loader, num_epochs=100):
    train_losses = []
    val_losses = []
    train_accs = []
    val_accs = []
    best_val_loss = float('inf')
    patience_counter = 0
    patience = 15

    for epoch in range(num_epochs):
        # Training phase
        model.train()
        train_loss = 0.0
        train_correct = 0
        train_total = 0

        for batch_X, batch_y in train_loader:
            batch_X, batch_y = batch_X.to(device), batch_y.to(device)

            optimizer.zero_grad()
            outputs = model(batch_X)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()

            train_loss += loss.item()

            # Calculate accuracy
            _, predicted = torch.max(outputs.data, 1)
            train_total += batch_y.size(0)
            train_correct += (predicted == batch_y).sum().item()

        # Validation phase
        model.eval()
        val_loss = 0.0
        val_correct = 0
        val_total = 0

        with torch.no_grad():
            for batch_X, batch_y in test_loader:
                batch_X, batch_y = batch_X.to(device), batch_y.to(device)
                outputs = model(batch_X)
                loss = criterion(outputs, batch_y)
                val_loss += loss.item()

                # Calculate accuracy
                _, predicted = torch.max(outputs.data, 1)
                val_total += batch_y.size(0)
                val_correct += (predicted == batch_y).sum().item()

        train_loss /= len(train_loader)
        val_loss /= len(test_loader)
        train_acc = 100 * train_correct / train_total
        val_acc = 100 * val_correct / val_total

        train_losses.append(train_loss)
        val_losses.append(val_loss)
        train_accs.append(train_acc)
        val_accs.append(val_acc)

        # Learning rate scheduling
        scheduler.step(val_loss)

        # Early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            patience_counter = 0
            # Save best model
            torch.save(model.state_dict(), 'best_bench_press_lstm.pth')
        else:
            patience_counter += 1

        if epoch % 10 == 0:
            print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%')

        if patience_counter >= patience:
            print(f"Early stopping at epoch {epoch+1}")
            break

    return train_losses, val_losses, train_accs, val_accs

# Train the model
print("Starting model training...")
train_losses, val_losses, train_accs, val_accs = train_model(model, train_loader, test_loader)

# Load best model
model.load_state_dict(torch.load('best_bench_press_lstm.pth'))

# Evaluation
model.eval()
test_predictions = []
test_targets = []

with torch.no_grad():
    for batch_X, batch_y in test_loader:
        batch_X, batch_y = batch_X.to(device), batch_y.to(device)
        outputs = model(batch_X)
        _, predicted = torch.max(outputs.data, 1)
        probs = torch.softmax(outputs, dim=1)   # [batch_size, 2]
        probs_class1 = probs[:, 1]
        print(probs_class1)
        print(batch_y)
        test_predictions.append(predicted.cpu().numpy())
        test_targets.append(batch_y.cpu().numpy())

# Combine all predictions and targets
test_predictions = np.concatenate(test_predictions, axis=0)
test_targets = np.concatenate(test_targets, axis=0)

# Calculate metrics
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

accuracy = accuracy_score(test_targets, test_predictions)
precision = precision_score(test_targets, test_predictions, zero_division=0)
recall = recall_score(test_targets, test_predictions, zero_division=0)
f1 = f1_score(test_targets, test_predictions, zero_division=0)
conf_matrix = confusion_matrix(test_targets, test_predictions)

print(f"\nTest Results:")
print(f"Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-Score: {f1:.4f}")
print(f"\nConfusion Matrix:")
print(conf_matrix)
print(f"  [[TN FP]")
print(f"   [FN TP]]")

# Save scaler
joblib.dump(scaler_X, 'scaler_X.pkl')

print("\nModel and scaler saved!")
print("- Model: best_bench_press_lstm.pth")
print("- Scaler: scaler_X.pkl")

for i in range(len(test_dataset)):
  output = model(test_dataset[i][0].to(device))
  print(f'test signal {i}')
  if torch.max(output.data, 0).indices.to('cpu') == test_dataset[i][1]:
    if test_dataset[i][1] == 1:
      print('correctly predicted failure')
    else:
      print('correctly predicted non-failure')

  else:
    if test_dataset[i][1] == 1:
      print('predicted non-failure, but should be failure')
    else:
      print('predicted failure, but should be non-failure')

import matplotlib.pyplot as plt

plt.plot(test_dataset[69][0])

from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

# Collect probabilities for class 1 (fatigue/failure)
all_probs = []
all_targets = []

with torch.no_grad():
    for batch_X, batch_y in test_loader:
        batch_X = batch_X.to(device)
        outputs = model(batch_X)                # [batch_size, 2]
        probs = torch.softmax(outputs, dim=1)   # [batch_size, 2]
        probs_class1 = probs[:, 1].cpu().numpy()# Probability for class 1
        all_probs.append(probs_class1)
        all_targets.append(batch_y.cpu().numpy())

# Flatten lists to arrays
import numpy as np
all_probs = np.concatenate(all_probs)
all_targets = np.concatenate(all_targets)

all_probs

all_targets

fpr, tpr, thresholds = roc_curve(all_targets, all_probs) # all_targets: shape [N], all_probs: shape [N]
roc_auc = auc(fpr, tpr)

plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.0])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()